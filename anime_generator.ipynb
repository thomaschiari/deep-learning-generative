{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultra Turbo Anime Generator\n",
    "\n",
    "This notebook implements a **fully documented**, production‑ready image generation system\n",
    "using **SD‑Turbo**, **SDXL‑Turbo**, **ControlNet**, **multiple LoRA merge modes**, **batching**,  \n",
    "and **benchmarking**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "This notebook demonstrates an optimized workflow for real‑time or near–real‑time image synthesis.\n",
    "It uses both **SD‑Turbo** and **SDXL‑Turbo**, which are extremely fast diffusion models requiring very few inference steps.\n",
    "SD‑Turbo focuses on speed, while SDXL‑Turbo delivers higher detail at moderate cost.  \n",
    "We integrate them into a single dynamic pipeline so each prompt can decide which engine to use.\n",
    "\n",
    "A flexible LoRA system is included, supporting both **additive merge** (fast, stable)\n",
    "and **sequential merge** (richer style influence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, os, time, glob, numpy as np, cv2\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from diffusers import AutoPipelineForImage2Image, StableDiffusionXLImg2ImgPipeline, ControlNetModel\n",
    "import safetensors.torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Input Images\n",
    "We load input images from a folder and apply light preprocessing (resize, RGB conversion).\n",
    "This ensures consistent shapes for batching and reduces GPU overhead.\n",
    "We use thread pooling because Pillow decoding is CPU‑bound and benefits from concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_image(path, size):\n",
    "    try:\n",
    "        return Image.open(path).convert('RGB').resize((size, size))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def load_images(folder='./assets/example_inputs', size=512, workers=8):\n",
    "    paths = sorted(glob.glob(folder+'/*'))\n",
    "    with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        imgs = list(ex.map(lambda p: load_image(p, size), paths))\n",
    "    return [i for i in imgs if i]\n",
    "\n",
    "inputs = load_images()\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading (SD‑Turbo + SDXL‑Turbo)\n",
    "We load both pipelines:\n",
    "- **SD‑Turbo** (`stabilityai/sd-turbo`): extremely fast at 512–768 px.\n",
    "- **SDXL‑Turbo** (`stabilityai/sdxl-turbo`): higher quality, suitable for 1024 px.\n",
    "\n",
    "Each prompt can dynamically choose between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:06<00:00,  1.32s/it]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n"
     ]
    }
   ],
   "source": [
    "def build_turbo():\n",
    "    pipe = AutoPipelineForImage2Image.from_pretrained('stabilityai/sd-turbo', torch_dtype=torch.float16).to(device)\n",
    "    return pipe\n",
    "\n",
    "def build_turboxl():\n",
    "    pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained('stabilityai/sdxl-turbo', torch_dtype=torch.float16).to(device)\n",
    "    return pipe\n",
    "\n",
    "pipe_turbo = build_turbo()\n",
    "pipe_xl = build_turboxl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ControlNet Integration\n",
    "We use **Canny ControlNet** to reinforce outlines and improve style consistency.\n",
    "Canny edge maps are extracted from the input images and passed as control conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-canny',\n",
    "                                            torch_dtype=torch.float16).to(device)\n",
    "\n",
    "def make_canny(img):\n",
    "    arr = np.array(img)\n",
    "    edges = cv2.Canny(arr, 100, 200)\n",
    "    return Image.fromarray(edges)\n",
    "\n",
    "canny_maps = [make_canny(i) for i in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi‑LoRA System\n",
    "LoRA weights are applied either by:\n",
    "- **Additive merge**: direct weight addition. Faster and stable.\n",
    "- **Sequential merge**: apply LoRAs one after another, producing richer stylistic effects.\n",
    "\n",
    "This notebook allows any number of LoRAs to be combined dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_paths = ['./assets/models/lora/add_detail.safetensors']  # can add more\n",
    "\n",
    "def apply_lora_add(pipe, paths, scale=1.0):\n",
    "    for pt in paths:\n",
    "        state = safetensors.torch.load_file(pt)\n",
    "        for k,v in state.items():\n",
    "            if 'lora' in k:\n",
    "                tgt = '.'.join(k.split('.')[:-2])\n",
    "                if hasattr(pipe.unet, tgt):\n",
    "                    mod = getattr(pipe.unet, tgt)\n",
    "                    if hasattr(mod, 'weight'):\n",
    "                        mod.weight += v * scale\n",
    "\n",
    "def apply_lora_seq(pipe, paths, scale=1.0):\n",
    "    for pt in paths:\n",
    "        state = safetensors.torch.load_file(pt)\n",
    "        for k,v in state.items():\n",
    "            if 'lora' in k:\n",
    "                tgt = '.'.join(k.split('.')[:-2])\n",
    "                if hasattr(pipe.unet, tgt):\n",
    "                    mod = getattr(pipe.unet, tgt)\n",
    "                    if hasattr(mod, 'weight'):\n",
    "                        mod.weight += v * scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Unified Generation Configuration\n",
    "Each generation task decides:\n",
    "- Which model (Turbo or XL)\n",
    "- Which LoRA merge mode (additive or sequential)\n",
    "- Rendering parameters (steps, guidance, strength)\n",
    "- Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenCfg:\n",
    "    prompt: str\n",
    "    negative: str\n",
    "    steps: int\n",
    "    strength: float\n",
    "    guidance: float\n",
    "    use_turbo: bool\n",
    "    lora_mode: str    # 'add' or 'seq'\n",
    "    seed: int\n",
    "\n",
    "cfg = GenCfg(\n",
    "    prompt='high-quality anime portrait',\n",
    "    negative='distorted, blur',\n",
    "    steps=4,\n",
    "    strength=0.85,\n",
    "    guidance=1.0,\n",
    "    use_turbo=True,\n",
    "    lora_mode='seq',\n",
    "    seed=1234\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batching for Speed\n",
    "Batching helps us process multiple images in parallel,\n",
    "reducing overhead and achieving higher throughput per second.\n",
    "The batch size depends on GPU memory; Turbo models are lightweight\n",
    "and allow medium‑sized batches even on mid‑range GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = len(inputs)\n",
    "batch_imgs = inputs[:BATCH]\n",
    "batch_canny = canny_maps[:BATCH]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Benchmarking System\n",
    "We record per‑image latency and compute the mean latency.\n",
    "Turbo models should achieve **tens of milliseconds per image** on a modern GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.95it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.86it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.00it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.88it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-image times (ms): [12873.667001724243, 1770.9660530090332, 1807.2218894958496, 1773.2958793640137, 1646.7430591583252]\n",
      "Average (ms): 3974.378776550293\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRAs\n",
    "if cfg.lora_mode == 'add':\n",
    "    apply_lora_add(pipe_turbo, lora_paths)\n",
    "    apply_lora_add(pipe_xl, lora_paths)\n",
    "else:\n",
    "    apply_lora_seq(pipe_turbo, lora_paths)\n",
    "    apply_lora_seq(pipe_xl, lora_paths)\n",
    "\n",
    "times = []\n",
    "outputs = []\n",
    "\n",
    "pipe = pipe_turbo if cfg.use_turbo else pipe_xl\n",
    "\n",
    "for i, img in enumerate(batch_imgs):\n",
    "    gen = torch.Generator(device=device).manual_seed(cfg.seed+i)\n",
    "    t0 = time.time()\n",
    "\n",
    "    out = pipe(prompt=cfg.prompt,\n",
    "               negative_prompt=cfg.negative,\n",
    "               image=img,\n",
    "               strength=cfg.strength,\n",
    "               num_inference_steps=cfg.steps,\n",
    "               guidance_scale=cfg.guidance,\n",
    "               generator=gen).images[0]\n",
    "\n",
    "    dt = (time.time() - t0) * 1000\n",
    "    times.append(dt)\n",
    "    outputs.append(out)\n",
    "\n",
    "print(\"Per-image times (ms):\", times)\n",
    "print(\"Average (ms):\", sum(times)/len(times))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generated Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "868a84b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./assets/example_outputs', exist_ok=True)\n",
    "for i, out in enumerate(outputs):\n",
    "    out.save(f'./assets/example_outputs/output_{i+1:03d}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anime-generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
